{
  "paragraphs": [
    {
      "text": "%spark \nimport kafka.serializer.StringDecoder\n//import com.datastax.spark.connector.streaming._\n//import org.apache.spark.streaming.{Seconds, StreamingContext}\n//import org.apache.spark.{TaskContext, SparkConf}\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka.{OffsetRange, HasOffsetRanges, KafkaUtils}\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\n\nimport sqlContext.implicits._\nimport spark.implicits._\nimport java.util.UUID\n\nval ssc \u003d new StreamingContext(sc, Seconds(5))\nimport com.datastax.spark.connector.streaming._\n//val sqlContext \u003d new SQLContext(sc)",
      "dateUpdated": "Aug 31, 2016 3:30:49 PM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1472629185346_-1341830746",
      "id": "20160726-065534_1308137064",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nimport kafka.serializer.StringDecoder\n\nimport org.apache.spark.streaming._\n\nimport org.apache.spark.streaming.kafka.{OffsetRange, HasOffsetRanges, KafkaUtils}\n\nimport org.apache.spark.sql._\n\nimport org.apache.spark.sql.types._\n\nimport org.apache.spark.sql.functions._\n\nimport sqlContext.implicits._\n\nimport spark.implicits._\n\nimport java.util.UUID\n\nssc: org.apache.spark.streaming.StreamingContext \u003d org.apache.spark.streaming.StreamingContext@6bd29edd\n\nimport com.datastax.spark.connector.streaming._\n"
      },
      "dateCreated": "Aug 31, 2016 7:39:45 AM",
      "dateStarted": "Aug 31, 2016 3:30:49 PM",
      "dateFinished": "Aug 31, 2016 3:30:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark \n  val generateUUID \u003d udf(() \u003d\u003e UUID.randomUUID().toString)    \n\n  val topicsSet \u003d Set(\"my_topic\")\n  val brokers \u003d sys.env(\"KAFKA_ADDRESS\")\n  val kafkaParams \u003d Map[String, String](\"metadata.broker.list\" -\u003e brokers)\n  val messages \u003d KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicsSet)\n  val schema \u003d StructType(StructField(\"name\",StringType,false)::StructField(\"value\", IntegerType, false)::StructField(\"time\", TimestampType, false)::Nil)\n  //sqlContext.createExternalTable(\"bTable\", \"parquet\", schema, Map(\"path\" -\u003e \"/usr/zeppelin/data/\"))\n  //val df \u003d sqlContext.createExternalTable(\"bigTable\",\"/usr/zeppelin/data/bigTable\")\n  //df.schema(schema)\n  messages.foreachRDD { (rdd,time) \u003d\u003e\n//        println(time)\n        val t \u003d time.milliseconds\n        val dataDF \u003d sqlContext.read.json(rdd.map(_._2)).withColumn(\"time\", lit(t).cast(\"timestamp\")).withColumn(\"id\", generateUUID())\n//        dataDF.printSchema()\n        dataDF.createOrReplaceTempView(\"mytable\")\n        \n        dataDF.write.format(\"parquet\").mode(\"append\").save(\"/usr/zeppelin/data/datalake.parquet\")\n        val allValues \u003d sqlContext.sql(\"select name as prod, value, time as timestamp, id  from mytable \")\n        allValues.write.format(\"org.apache.spark.sql.cassandra\").option(\"header\",\"false\").mode(\"append\").options(Map( \"table\" -\u003e \"all_results\", \"keyspace\" -\u003e \"mio\")).save()\n        val averageValues \u003d sqlContext.sql(\"select name as prod, AVG(value) as average, first(time) as timestamp from mytable group by name\")\n        averageValues.show()\n        averageValues.write.format(\"org.apache.spark.sql.cassandra\").option(\"header\",\"false\").mode(\"overwrite\").options(Map( \"table\" -\u003e \"results\", \"keyspace\" -\u003e \"mio\")).save()\n//        sqlContext.parquetFile( \"/usr/zeppelin/data/datalake.parquet\" )\n//          .coalesce(6)\n//          .write.parquet(\"/usr/zeppelin/data/all.parquet\")\n  }\n// Start the computation\nssc.start()\n//ssc.awaitTerminationOrTimeout(30 * 1000)\n",
      "dateUpdated": "Aug 31, 2016 3:30:49 PM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "graph": {
          "mode": "table",
          "height": 111.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1472629185386_-1357220702",
      "id": "20160726-072729_926033663",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\ngenerateUUID: org.apache.spark.sql.expressions.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction0\u003e,StringType,Some(List()))\n\ntopicsSet: scala.collection.immutable.Set[String] \u003d Set(my_topic)\n\nbrokers: String \u003d kafka:9092\n\nkafkaParams: scala.collection.immutable.Map[String,String] \u003d Map(metadata.broker.list -\u003e kafka:9092)\n\nmessages: org.apache.spark.streaming.dstream.InputDStream[(String, String)] \u003d org.apache.spark.streaming.kafka.DirectKafkaInputDStream@5ada0006\n\nschema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(name,StringType,false), StructField(value,IntegerType,false), StructField(time,TimestampType,false))\n"
      },
      "dateCreated": "Aug 31, 2016 7:39:45 AM",
      "dateStarted": "Aug 31, 2016 3:30:51 PM",
      "dateFinished": "Aug 31, 2016 3:31:02 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql select name as prod, value, time as timestamp, id  from mytable",
      "dateUpdated": "Aug 31, 2016 3:30:49 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "helium": {},
        "graph": {
          "mode": "table",
          "height": 168.0,
          "optionOpen": false,
          "keys": [],
          "values": [
            {
              "name": "value",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "yAxis": {
              "name": "value",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1472629185388_-1359529196",
      "id": "20160726-094150_302242907",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Table or view not found: mytable; line 1 pos 56\nset zeppelin.spark.sql.stacktrace \u003d true to see full stacktrace"
      },
      "dateCreated": "Aug 31, 2016 7:39:45 AM",
      "dateStarted": "Aug 31, 2016 3:30:59 PM",
      "dateFinished": "Aug 31, 2016 3:31:02 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark \nval mydf \u003dsqlContext.sql(\"select * from mytable\")\nmydf.show()\n//ssc.awaitTerminationOrTimeout(30 * 1000)\n//ssc.stop(stopSparkContext\u003dtrue, stopGracefully\u003dtrue)",
      "dateUpdated": "Aug 31, 2016 3:30:49 PM",
      "config": {
        "tableHide": true,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1472629185389_-1359913945",
      "id": "20160726-082126_24115141",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.sql.AnalysisException: Table or view not found: mytable; line 1 pos 14\n  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:449)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:468)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:453)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:453)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:443)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n  at scala.collection.immutable.List.foldLeft(List.scala:84)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:65)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:63)\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:51)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)\n  at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)\n  ... 60 elided\n"
      },
      "dateCreated": "Aug 31, 2016 7:39:45 AM",
      "dateStarted": "Aug 31, 2016 3:31:02 PM",
      "dateFinished": "Aug 31, 2016 3:31:03 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark ",
      "dateUpdated": "Aug 31, 2016 3:30:49 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1472629185390_-1358759698",
      "id": "20160726-144125_667601238",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Aug 31, 2016 7:39:45 AM",
      "dateStarted": "Aug 31, 2016 3:31:03 PM",
      "dateFinished": "Aug 31, 2016 3:31:03 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Streaming2",
  "id": "2BVH2Y24H",
  "lastReplName": {
    "value": "spark"
  },
  "angularObjects": {
    "2BW92AJ26:shared_process": [],
    "2BUE7YEUC:shared_process": [],
    "2BWYTCJZC:shared_process": [],
    "2BX6TPWNA:shared_process": [],
    "2BVS7W5AE:shared_process": [],
    "2BV15NK82:shared_process": [],
    "2BUVCFMWC:shared_process": [],
    "2BX44T9KJ:shared_process": [],
    "2BUQNY71C:shared_process": []
  },
  "config": {},
  "info": {}
}